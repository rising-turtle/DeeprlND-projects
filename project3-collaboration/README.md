[//]: # (References)

<!-- IMAGES -->
[gif_project_3_tennis_agents]: imgs/gif_project_3_tennis_agent.gif
[img_tennis_environment_observations]: imgs/img_tennis_environment_observations.png
[img_tennis_environment_actions]: imgs/img_tennis_environment_actions.png
[img_tensorboard_sample]: imgs/img_tensorboard_sample.png

[url_report]: https://github.com/wpumacay/DeeprlND-projects/blob/master/project3-collaboration/REPORT.md
[url_setup_script]: https://github.com/wpumacay/DeeprlND-projects/blob/master/project3-collaboration/setup.py
[url_trainer_script]: https://github.com/wpumacay/DeeprlND-projects/blob/master/project3-collaboration/trainer.py
[url_tensorboardX]: https://github.com/lanpa/tensorboardX

<!-- https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Brains.md#brain-properties -->

# Project 3: Collaboration and Competition

This project consists of an implementation of the **MADDPG** algorithm to control the
two agents from the **Tennis ml-agents** environment. We model the problem as a
multi-agent reinforcement learning setup and our implementation is based on the
MADDPG algorithm from the paper [**Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments**](https://arxiv.org/pdf/1706.02275.pdf)
by Lowe, et. al.. Below we show the trained agents' policies successfully controlling
both agents such that they pass the ball over and avoid dropping it or throwing it
out of bounds (get the longest rally possible).

![project-3-tennis-agents][gif_project_3_tennis_agents]

For further details about the algorithm, agent implementation and some results please 
refer to the accompanying report ([REPORT.md][url_report]).

## 1. Environment description

The environment chosen for the project is a **modified version of the Tennis Environment** 
from the Unity ML-Agents toolkit. The original version can be found [here](https://github.com/Unity-Technologies/ml-agents/blob/9b1a39982fd03de8f40f85d61f903e6d972fd2cc/docs/Learning-Environment-Examples.md#tennis),
and the version we will work with consists of a custom build provided by Udacity 
with the following description:

* A group of 2 agents (colored rackets) and a ball (orange sphere). The agents try
  to keep the ball from falling and try to pass it as much as they can.

* Each agent receives an **observation** consisting of a 24-dimensional vector. This
  local observation (per agent) is generated by an internal observation of a 8-dimensional
  vector stacked 3 times along timesteps (hence 24-d).

* Each agent can move around by applying **actions** consisting of speed factors
  applied along the x and y axes, giving a 2d action vector per agent.

* Each agent gets a **reward** of +0.1 each time it passes the ball over to the other
  side of the net, and reward of -0.01 if it lets the ball fall or if it hits it out
  of bounds. The environment is cooperative in the sense that all agents try to get
  as much reward as possible without getting in each other's way, and making life easier
  for the other agent. This environment is considered solved once the average over 
  episodes of the maximum reward obtained among both agent is maintained above +0.5 
  over 100 episodes.

* The task is **episodic**, and it resets either by reaching the maximum number of
  steps (25000) or by the ball being drop to the floor or thrown out of bounds.

### 1.1 Observation-space

Each agent receives a local observation consisting of a 8d vectors stacked over 3 
timesteps, which give an observation space of 24d vectors. Each of the features of 
the stacked 8d vectors is defined by the following measurements:

* X,Y "relative" position and velocity of the agent with respect to the base position 
  of the court. Note that the positions are relative in the for the agent to the 
  left (blue), but are kind of "relative" for the agent to the right (red). The x 
  component is flipped for the later, as we will see a few lines below in a snippet 
  from the ml-agents implementation.

* X,Y "relative" position and velocity of the ball with respect to the base position
  of the court. Note that in this case we also have the true measurement flipped in
  the x direction for the red agent.

All these measurements account for the 3x8d shape of the observation space. Below
we show a figure depicting these measurements.

![tennis-env-observations][img_tennis_environment_observations]

These measurements can be double-checked with the definition of the agent in the 
ml-agents toolkit, which can be found [here](https://github.com/Unity-Technologies/ml-agents/blob/9b1a39982fd03de8f40f85d61f903e6d972fd2cc/UnitySDK/Assets/ML-Agents/Examples/Tennis/Scripts/TennisAgent.cs#L43).
Below we show a small snippet from their C# agent implementation.

```Csharp
    public override void CollectObservations()
    {
        AddVectorObs(invertMult * (transform.position.x - myArea.transform.position.x));
        AddVectorObs(transform.position.y - myArea.transform.position.y);
        AddVectorObs(invertMult * agentRb.velocity.x);
        AddVectorObs(agentRb.velocity.y);

        AddVectorObs(invertMult * (ball.transform.position.x - myArea.transform.position.x));
        AddVectorObs(ball.transform.position.y - myArea.transform.position.y);
        AddVectorObs(invertMult * ballRb.velocity.x);
        AddVectorObs(ballRb.velocity.y);
    }
```

### 1.2 Actions-space

The actions that the agent can take consist of a 2d vector of speed factors along
the x and y axes, defined as follows:

* Mov<sup>A</sup><sub>x</sub>, Mov<sup>A</sup><sub>y</sub>: speed factors for agent A
  along x and y axes respectively. Both in the range of [-1,+1].

* Mov<sup>B</sup><sub>x</sub>, Mov<sup>B</sup><sub>y</sub>: speed factors for agent B
  along x and y axes respectively. Both in the range of [-1,+1]. Notice that for 
  this agent the x-axis factor is flipped to ensure that the range [-1,+1] has a similar
  meaning for both agents, namely: -1 forces to move away of the net, and +1 forces
  to move towards the net.


The figure below shows these actions a bit more clearly:

![tennis-env-actions][img_tennis_environment_actions]

Also, we can double check the meaning of these actions from the ml-agents implementation 
of the Tennis agent in C#, which you can find [here](https://github.com/Unity-Technologies/ml-agents/blob/9b1a39982fd03de8f40f85d61f903e6d972fd2cc/UnitySDK/Assets/ML-Agents/Examples/Tennis/Scripts/TennisAgent.cs#L57). 
Below we show a snippet of the implementation (notice the *invertMult* factor, which
flips various quantities for agent B):

```CSharp
    public override void AgentAction(float[] vectorAction, string textAction)
    {
        var moveX = Mathf.Clamp(vectorAction[0], -1f, 1f) * invertMult;
        var moveY = Mathf.Clamp(vectorAction[1], -1f, 1f);
        
        if (moveY > 0.5 && transform.position.y - transform.parent.transform.position.y < -1.5f)
        {
            agentRb.velocity = new Vector3(agentRb.velocity.x, 7f, 0f);
        }

        agentRb.velocity = new Vector3(moveX * 30f, agentRb.velocity.y, 0f);

        if (invertX && transform.position.x - transform.parent.transform.position.x < -invertMult || 
            !invertX && transform.position.x - transform.parent.transform.position.x > -invertMult)
        {
                transform.position = new Vector3(-invertMult + transform.parent.transform.position.x, 
                                                            transform.position.y, 
                                                            transform.position.z);
        }

        textComponent.text = score.ToString();
    }
```

### 1.3 Environment dynamics and rewards

Both agents and the ball spawn at the same time at some initial random positions 
within a certain range. As mentioned earlier, each agent receives a reward of
+0.1 each time it successfully passes the ball over the net to the other side (check C# 
implementation [here](https://github.com/Unity-Technologies/ml-agents/blob/9b1a39982fd03de8f40f85d61f903e6d972fd2cc/UnitySDK/Assets/ML-Agents/Examples/Tennis/Scripts/HitWall.cs#L22)).
They are penalized with a reward of -0.01 each time they let the ball fall to the 
floor in their side of the court or throw the ball out of bounds (check the C# implementation
[here](https://github.com/Unity-Technologies/ml-agents/blob/9b1a39982fd03de8f40f85d61f903e6d972fd2cc/UnitySDK/Assets/ML-Agents/Examples/Tennis/Scripts/HitWall.cs#L39)).

The environment is episodic as it resets every time the ball is dropped or thrown
out of bounds. There's also a time step limit of 25000 steps. The environment is
considered **solved** once both agents can successfully maintain an average reward
(max over both agent's scores per episode) of +0.5 over 100 episodes.

## 2. Setting up the code

In this section we give instructions about how to use the code of this implementation.
Unlike the previous projects, we haven't abstracted away much of the functionality, 
and went for a minimal implementation of the MADDPG algorithm in a single file (we will
port some of the DDPG abstractions from the previous project in later updates). However, 
there are some dependencies and a small gym-like environment wrapper for the unity ml-agents
environment that needs to be installed. These requirements can be easily setup with the
[setup.py][url_setup_script] file we provide, as we will see later.

### 2.1 Downloading the environment

The environment provided is a custom build of the ml-agents Tennis environment (the original
has 18 copies of the environment running in the same scene), with the features described 
in the earlier section. The environment is provided as an executable which we can download 
from the links below according to our platform:

Platform | Link
-------- | -----
Linux             | [Link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)
Mac OSX           | [Link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)
Windows (32-bit)  | [Link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)
Windows (64-bit)  | [Link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)

Keep the .zip file where you downloaded it for now. We will later explain where
to extract its contents when finishing the setup.

### **Note**

The executables provided are compatible only with an older version of the ML-Agents
toolkit (version 0.4.0). The setup below will take care of this and install the
appropriate version of the ml-agents API, but keep this in mind if you want to use 
an executable on your own (either a custom build or the original ml-agents tennis 
environment).

### 2.2 Dependencies

As already mentioned, the environment provided is an instance of an ml-agents
environment and as such it requires the **appropriate** ml-agents python API to be
installed in order to interact with it. There are also some other required dependencies, 
so to facilitate installation all this setup is handled by the [setup.py][url_setup_script] 
script from the accompanying code, which we will discuss below. Also, there is no need
to download the Unity Editor for this project (unless you want to do a custom build
for your platform) because the builds for the appropriate platforms are already
provided for us.

### 2.3 Downloading the accompanying code and finishing setup

* Grab the accompanying code from this repo and navigate to the folder of project-3.

```bash
# clone the repo
git clone https://github.com/wpumacay/DeeprlND-projects
# go to the project3-collaboration folder
cd DeeprlND-projects/project3-collaboration
```

* (Suggested) Create an environment using a virtual environment manager like 
  [pipenv](https://docs.pipenv.org/en/latest/) or [conda](https://conda.io/en/latest/).

```bash
# create a virtual env. using conda
conda create -n deeprl_collaboration python=3.6
# activate the environment
source activate deeprl_collaboration
```

* Install [pytorch](https://pytorch.org/get-started/locally/#start-locally).

```bash
# Option 1: install pytorch (along with cuda). No torchvision, as we do not require it for this project.
conda install pytorch cudatoolkit=9.0 -c pytorch
# Option 2: install using pip. No torchvision, as we do not require it for this project.
pip install torch
```

* Install remaining requirements (libraries and our env-wrapper) using the provided
  [setup.py][url_setup_script] file (make sure you are in the same folder where this 
  file is located). This can be achieved by using pip, as shown below.

```bash
# install the ccontrol package and its dependencies using pip (dev mode to allow changes)
pip install -e .
```

* Finally, uncompress the executable downloaded previously into the executables folder in
  the repository

```bash
cd executables/
# copy the executable into the executables folder in the repository
cp {PATH_TO_ZIPPED_EXECUTABLE}/Tennis_Linux.zip ./
# unzip it
unzip Tennis_Linux.zip
```

* (Optional) In case you run into issues with **protobuf** due to tensorboardX (used
  for training) then just uninstall protobuf and install it again. This is caused by
  a mismatch in the versions installed by the unityagents=0.4.0 package and the one 
  required by the tensorboardX package.

```bash
# uninstall protobuf
pip uninstall protobuf
# install protobuf again
pip install protobuf
```

## 3. Training and testing

To train the agent we provided a trainer script ([trainer.py][url_trainer_script]) 
which you can use in the following way:

```bash
python maddpg_tennis.py train --OPTIONAL-FLAGS=OPTIONAL-FLAGS-VALUES
```

This requests the trainer to run a training session using options passed through
the command line (unlike previous projects where we used a better mechanism by using
gin-config), which include the following options:

Option                   | Meaning                                                                | Default value
-------------------------|------------------------------------------------------------------------|---------------
sessionId                | name of the training session (used for results prefixes)               | session_0
hp_replay_buffer_size    | size of the replay memory                                              | 1000000
hp_batch_size            | batch size of data to grab for each learning step                      | 256
hp_lrate_actor           | learning rate used for actor network                                   | 0.001
hp_lrate_critic          | learning rate used for the critic network                              | 0.001
hp_tau                   | soft update factor used for target-networks updates (polyak averaging) | 0.001
hp_train_update_freq     | how frequently (in steps) should we request the learning step          | 4
hp_train_num_updates     | how many updates/passes to be done during a learning step              | 2

There are other training configurable parameters which are not exposed through the
command line and can be manually configured through in the trainer script. These
include:

```python
# training parameters (not exposed through the command line)
NUM_AGENTS              = 2         # number of agents in the multiagent env. setup
GAMMA                   = 0.99      # discount factor applied to the rewards
LOG_WINDOW              = 100       # size of the smoothing window and logging window
TRAINING_EPISODES       = 50000     # number of training episodes
MAX_STEPS_IN_EPISODE    = 3000      # maximum number of steps in an episode
SEED                    = 200         # random seed to be used
EPSILON_SCHEDULE        = 'linear'  # type of shedule 
EPSILON_DECAY_FACTOR    = 0.999     # decay factor for e-greedy geometric schedule
EPSILON_DECAY_LINEAR    = 2e-5      # decay factor for e-greedy linear schedule
TRAINING_STARTING_STEP  = int(5e4)  # step index at which training should start
```

The training results are stored in the **results** folder, and these are further
stored inside a folder created by the trainer whose name is given by the **sessionId**
parameter passed through the command line. The saved results consist of the following :

* The weights of all the trained models: both actor and critic models for all agents
  (two in our case) are saved in the folder created for the session and with names 
  **checkpoint_actor_#AGENT.pth** and **checkpoint_critic_#AGENT.pth** respectively,
  for example:

  * Agent-1: **./results/session_default/checkpoint_actor_0.pth**, **./results/session_default/checkpoint_critic_0.pth**
  * Agent-2: **./results/session_default/checkpoint_actor_1.pth**, **./results/session_default/checkpoint_critic_1.pth**

* The training logs are stored in a **tensorboard** file (using [tensorboardX][url_tensorboardX]
  as library for interoperatibility with tensorboard) located inside a folder with
  name *tensorboard_summary*, for example: 

  * Summary: **./results/session_default/tensorboard_summary/events.out.tfevents.1563313949.labpc**

To test the trained agent just run the trainer script in test mode as follows:

```bash
python maddpg_tennis.py test --sessionID=YOUR_SESSION_ID
```

To run the pretrained agent provided as part of the project submission just run the 
following:

```bash
python maddpg_tennis.py test --sessionID=session_submission
```

Finally, to check the training logs using tensorboard just invoque tensorboard pointing
to the folder where the training logs are:

```bash
# invoque tensorboard there
tensorboard --logdir=./results/SOME_SESSION_NAME/tensorboard_summary
# or, invoque it with a specific port, in case you have multiple tensorboard sessions running
tensorboard --logdir=./results/SOME_SESSION_NAME/tensorboard_summary --port=SOME_PORT
```

For example, to check the training logs of the agent trained for the project submission, 
just do the following:

```bash
tensorboard --logdir=./results/session_submission/tensorboard_summary/
```

## References

* [*Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments* by Lowe et. al.](https://arxiv.org/pdf/1706.02275.pdf)
* [*Continuous control through deep reinforcement learning* paper by Lillicrap et. al.](https://arxiv.org/pdf/1509.02971.pdf)
* [*Deterministic Policy Gradients Algorithms* paper by Silver et. al.](http://proceedings.mlr.press/v32/silver14.pdf)
* [*DDPG implementation* from Udacity](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum)